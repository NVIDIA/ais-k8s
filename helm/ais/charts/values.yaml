#
# Default values.yaml for AIS deployment chart.
#
# A new deployment *will* have to modify some of these defaults - we can't know intended
# disk paths etc in advance.
#
# If using helm directly such over-rides may be best captured in a wrapper script to
# 'helm install' using --set and --set-string.
#
# A cleaner approach is to use the likes of ArgoCD and edit this values.yaml file
# as needed, managing changes in Git and applying them with ArgoCD.
#
# No support for Kustomize at this time.
#
# The values that likely will require attention in a new deployment are grouped
# together under 'aiscluster'. Others are less likely to need tweaking (and should
# probably be removed from this file and instead tweaked via Kustomize when needed).
#
# You'll also need to visit the monitoring section - tags, graphite, grafana,
# prometheus, external_monitoring.
#
# Instead of leaving blank values, the values in sections for 'aiscluster' and for
# monitoring (all preceding the big banner announcing the end of likely changes)
# are provided for a configuration as follows:
#
# - using ArgoCD for deployment, editing this value file and committing in Git
#   and driving updates from that
# - the application name in ArgoCD (release name in Helm) is 'foo'; the chart
#   and values don't ever mention that directly, but we need it in labeling
#   examples below
# - we'll label 20 nodes as target nodes (nvidia.com/ais-target=foo-ais)
#   [aiscluster.expected_target_nodes]; if we don't want to pre-declare the
#   intended number then set this to 0
# - we'll label those same 20 nodes [aiscluster.expected_proxy_nodes] to run electable proxies (eligible to be
#   primary proxy at any time, nvidia.com/ais-proxy=foo-ais-electable)
# - AIS cluster state (cached cluster map etc) will be stored under /etc/ais
#   on each node, exposed as a hostPath mount [aiscluster.hostpathPrefix]
# - Container images are sourced from private repos on quay.io using a pre-installed
#   secret named 'ais-pull-secret' which exist in the same namespace we intend to
#   deploy AIStore to [aiscluster.image.*]
# - All target nodes have 10 x distinct filesystems for our use, all mounted at
#   /ais/sd[a-j] on each node [aiscluster.target.mountPaths]
# - We will deploy internal monitoring - Graphite, Grafana, Prometheus
# - the chart will create a PV/PVC pair for both Graphite and Grafana, both using local
#   storage on target node 'aistore0756'
#

aiscluster:
  #
  # Expected number of target/proxy pods in deployment. Set to 0 to leave initial
  # cluster deployment to work this out for itself, but initial deployment
  # is smoother if you provide this guide.
  #
  expected_target_nodes: 20
  expected_proxy_nodes: 20
  #
  # Prefix used on aisnode host nodes for local storage for AIS cluster state.
  #
  hostpathPrefix: "/etc/ais"
  #
  # Determines if setting `AIS_HOST_IP` env variable should be skipped.
  # It might be useful in cases the `status.hostIP` is internal IP which cannot
  # be accessed by other pods.
  #
  skipHostIP: false
  #
  # Secrets containing backend provider credentials.
  # Secrets are populated manually
  #
  awsSecretName: ""
  gcpSecretName: ""
  azureSecretName: ""

  #
  # Container images, pull secret names, pull policy.
  #
  image:
    aisnode:
      repository: "aistore/aisnode"
      tag: "3.4"
    kubectl:
      repository: aistore/ais-init
      tag: latest
    pullPolicy: IfNotPresent
    pullSecretNames:
      # k8s secret name to use in pulling images; secret populated manually
      #- ais-pull-secret
  #
  # Ingress to the proxy/gateway service and to any Grafana instance
  #
  ingress:
    gateway:
      externalIP: ""                     # metallb external IP for ingress
      port:       51080                  # external ingress port
      targetPort: 51080                  # port used within pod
    grafana:
      externalIP: ""                     # metallb external IP for Grafana instance
      port:       3000                   # external ingress port
      targetPort: 3000                   # port used within pod
  #
  # If using external ingress into the AIS cluster then cluster_cidr must be
  # set to the pod CIDR range in use on k8s covering *all nodes* (ie not just
  # the podCIDR from any one node). If using Kubespray to deploy k8s this is
  # kube_pods_subnet in group_vars/k8s-cluster/k8s-cluster.yml.
  #
  # In playbooks we change kubelet to permit the unsafe somaxconn sysctl,
  # and we apply to aisnode pods with the value below.
  #
  k8s:
    cluster_cidr:           ""    # must be set when using ingress with metlalb
    container_capabilities:
      # Needed for debug if you wish to run delve within a pod (or look into kubesquash etc)
      #- SYS_PTRACE
    sysctls:
      # Set this to 100000 *if* you also change kubelet.env to include
      #   --allowed-unsafe-sysctls='net.core.somaxconn'
      # You *will* need this under extreme HTTP GET/PUT load
      somaxconn: 0
  #
  # Mount paths on each target node to be used for AIS bucket data. These should be
  # precreated filesystems, and no two paths must reside in the same filesystem.
  #
  target:
    # external access to target pods is via a hostPort - set this to 51081
    # (or other chosen hostport) is needed; will require privileged
    # containers
    hostPort: 0

    # Defines if specified mountpaths are supposed to be used for external persistent volume.
    externalVolumes: false
    # Each external volume will be this size. Omitted when externalVolumes == false.
    externalVolumesSize: 50Gi

    mountPaths:
      - /ais/sda
      - /ais/sdb
      - /ais/sdc
      - /ais/sdd
      - /ais/sde
      - /ais/sdf
      - /ais/sdg
      - /ais/sdh
      - /ais/sdi
      - /ais/sdj

#
# If you have an existing graphite installation then set the builtin_monitoring
# tag to false and supply the host (or IP) and port for graphite in
# map external_monitoring.
#
# If builtin_monitoring is true and you don't want this chart to install
# Prometheus then set the prometheus tag to false.
#
tags:
  builtin_monitoring: true
  prometheus:         true

#
# Alternatively, leave the builtin-monitoring tag true (the default) and
# we'll use subchart dependencies to deploy graphite and grafana within the k8s
# cluster.
#
# If data persistence is enabled for Graphite and Grafana then local storage
# must already have been assigned on the indicated node and path combinations
# below - we don't create the underlying storage here, we're just creating a PV
# from existing local storage to satisfy the PVC made from graphite and grafana.
#
# XXX TODO:
#
#   - would be nice to add some standard dashboards; do this via another sidecar
#

#
# Key paths here that match those of https://github.com/kiwigrid/helm-charts/tree/master/charts/graphite
# will over-ride default values in the graphite dependency. Local additions are all within the ais map.
#
# If 'persistence' is set to true then we require a PV/PVC. There are two choices:
#
# 1. Create a PV and PVC externally (i.e. outside of this chart) and simply quote the PVC
#    under 'existingClaim' below. Use whatever storage class etc you want. Leave ais.pv.path
#    as the empty string.
#
# 2. To have this chart create a simple local-storage PV at a specified path on a specified node
#    an a corresponding PVC leave the existingClaim as "ais-graphite-pvc" and complete the three
#    items under ais.pv.  NOTE: You cannot update these values once the PV/PVC pair is created.
#
graphite:
  persistence:
    enabled:       true
    existingClaim: ais-graphite-pvc
  ais:
    pv:
      capacity: "500Gi"
      path:     "/stats/graphite"
      node:     "aistore0756"

  affinity: {}

#
# Key paths here that match those of https://github.com/helm/charts/tree/master/stable/grafana
# will over-ride default values in the grafana dependency. Local additions are all within the ais map.
#
# See notes for graphite about regarding selecting storage for persistence.
#
grafana:
  persistence:
    enabled:       true
    existingClaim: ais-grafana-pvc
  ais:
    pv:
      capacity: "50Gi"
      path:     "/stats/grafana"
      node:     "aistore0756"
  service:
    type: NodePort
  sidecar:
    datasources:
      enabled: true
      label:   ais_grafana_datasource
    dashboards:
      enabled: false
      label:   ais_grafana_dashboard

#
# Key paths here that match those of https://github.com/helm/charts/tree/master/stable/prometheus
# will over-ride default values in the grafana dependency. Local additions are all within the ais map.
#
# XXX TODO enable persistence
#
prometheus:
  alertmanager:
    persistentVolume:
      enabled: false
  server:
    persistentVolume:
      enabled: false

#
# Used only if builtin_monitoring is over-ridden to false. No Grafana or Prometheus here - we
# just arrange to send AIS stats to Graphite, and the external provider is responsible for
# node metrics, visualization etc.
#
external_monitoring:
  graphite_host: somehost
  graphite_port: 2003

# ---------------------------------------------------------------------------------
# |                                                                               |
# |          Values below much less likely to need changing                       |
# |                                                                               |
# ---------------------------------------------------------------------------------

#
# "Common" ais.json config values - actually not genuinely common and applicable to
# all of proxy/target, but since those templates have duplicated
# sections we may as well avoid also duplicating values.
#
# Don't add anything here that does not contribute to ais.json; and only add
# parametrized elements to _ais_common.json - no literal values there.
#
ais_config_common:
  auth:
    enabled:     false
    secret:      NotInUse
  checksum:
    type: xxhash
    validate_cold_get: true
    validate_warm_get: false
    validate_obj_move: false
    enable_read_range: false
  client:
    # test value
    client_timeout:      120s
    client_long_timeout: 30m
    list_timeout:        10m
  compression:
    block_size: 262144
    checksum: false
  disk:
    disk_util_low_wm:  20
    disk_util_high_wm: 80
    disk_util_max_wm:  95
    iostat_time_long:  2s
    iostat_time_short: 100ms
  distributed_sort:
    compression:           never
    duplicated_records:    ignore
    missing_shards:        ignore
    ekm_malformed_line:    abort
    ekm_missing_key:       abort
    default_max_mem_usage: 80%
    dsorter_mem_threshold: 100GB
    call_timeout:          10m
  downloader:
    timeout: 1h
  ec:
    enabled:       false
    objsize_limit: 262144
    data_slices:   2
    parity_slices: 2
    batch_size:    64
    compression:   never
  fshc:
    enabled:       true
    test_files:    4
    error_limit:   2
  keepalivetracker:
    proxy:
      interval: 10s
      name:     heartbeat
      factor:   3
    target:
      interval: 10s
      name:     heartbeat
      factor:   3
    retry_factor: 5
    timeout_factor: 3
  log:
    level:     3
    max_size:  4194304
    max_total: 67108864
  lru:
    lowwm:             75
    highwm:            90
    out_of_space:      95
    dont_evict_time:   120m
    capacity_upd_time: 10m
    enabled:           true
  mirror:
    copies:            2
    burst_buffer:      512
    util_thresh:       0
    optimize_put:      false
    enabled:           false
  periodic:
    stats_time:      10s
    notif_time:      30s
    retry_sync_time: 2s
  rebalance:
    enabled:         true
    compression:     never
    dest_retry_time: 2m
    quiescent:       20s
    multiplier:      2
  timeout:
    cplane_operation:     2s
    max_keepalive:        4s
    max_host_busy:        20s
    startup_time:         1m
    send_file_time:       5m
  versioning:
    enabled:           true
    validate_warm_get: false

proxy:
  name: proxy        # A component label for selector
  config:
    log_dir: /var/log/ais
    test_fspaths:
      count:    0
      instance: 0
    net:
      hostname:               ""
      hostname_intra_control: ""
      hostname_intra_data:    ""
      l4:
        port:               51080
        port_intra_control: ""
        port_intra_data:    ""
        sndrcv_buf_size:    131072
      http:
        write_buffer_size: 0
        read_buffer_size:  0
        use_https:        false
        chunked_transfer: true
  service:
    type: ClusterIP
    port: 51080
  nodeSelector:
    key: nvidia.com/ais-proxy
  resources: {}
  # Apply the below node label on any node (just 1), the proxy runs on that node will become a primary at launch
  initialPrimaryProxyNodeLabel:
    name:  "nvidia.com/ais-initial-primary-proxy"


  # TODO: This should not be editable by a user. However, it requires bigger effort to implement a tool to merge
  # user-provided affinity with internal affinity. It's questionable effort when charts are meant to be
  # deprecated and replaced with operator.

  # Allow single proxy per Kubernetes node.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: component
                operator: In
                values:
                  - proxy
          topologyKey: topology.kubernetes.io/node


target:
  name: "target"   # A component label for selector
  config:
    log_dir: /var/log/ais
    test_fspaths:
      count:    0
      instance: 0
    nodiskio:
      enabled:    false
      dryobjsize: "8M"
    net:
      hostname:               ""
      hostname_intra_control: ""
      hostname_intra_data:    ""
      l4:
        port:               51081
        port_intra_control: 51082
        port_intra_data:    51083
        sndrcv_buf_size:    0
      http:
        write_buffer_size: 65536
        read_buffer_size:  65536
        use_https:         false
        chunked_transfer:  true
  service:
    port:     51081
    type: ClusterIP
  nodeSelector:
    key:   nvidia.com/ais-target
  resources: {}

  # TODO: This should not be editable by a user.
  # Allow single target per Kubernetes node.
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: component
                operator: In
                values:
                - target
          topologyKey: topology.kubernetes.io/node

#
# Values to describe admin container which should be deployed on one of the
# nodes in the cluster. By default it is disabled as it is not the integral
# part of the cluster but rather additional entity.
#
# Admin container is deployed on the node that is labeled with
# [nvidia.com/ais-admin=foo-ais] where `foo` is the deployment name.
#
admin:
  name: "admin"
  enabled: false
  image:
    repository: "aistore/admin"
    tag:        "3.4"
  nodeSelector:
    key: nvidia.com/ais-admin

#
# Note that out target/proxy/ne_proxy DaemonSets use any resource values from their
# respective values sections.
#
resources: {}

tolerations: {}
